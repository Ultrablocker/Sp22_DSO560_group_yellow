{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSO 560 Natural Language Processing Final Project\n",
    "\n",
    "### Team members: Tanner Curley, Shao Xuan Chew, Robert Zhu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: overall code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform lemmatization (not recommended when using any pretrained embedding)\n",
    "LEMMA = False\n",
    "# size of the vocab for\n",
    "VOCAB_SIZE = 20000\n",
    "# for self attention\n",
    "DATA_POINTS = 10\n",
    "DIMENSIONS = 240\n",
    "SEQUENCE_LENGTH = 4\n",
    "# use gpu\n",
    "USE_GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17358218745217873517\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7806648320\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1175065762901300741\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:07:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, LSTM, Flatten, concatenate, Activation, RepeatVector, Permute\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from keras_self_attention import SeqSelfAttention\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from textacy import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>product</th>\n",
       "      <th>narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>purchase order day shipping amount receive pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>forwarded message date tue subject please inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>retail_banking</td>\n",
       "      <td>forwarded message cc sent friday pdt subject f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report speciali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>credit_reporting</td>\n",
       "      <td>payment history missing credit report made mis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           product  \\\n",
       "0           0       credit_card   \n",
       "1           1       credit_card   \n",
       "2           2    retail_banking   \n",
       "3           3  credit_reporting   \n",
       "4           4  credit_reporting   \n",
       "\n",
       "                                           narrative  \n",
       "0  purchase order day shipping amount receive pro...  \n",
       "1  forwarded message date tue subject please inve...  \n",
       "2  forwarded message cc sent friday pdt subject f...  \n",
       "3  payment history missing credit report speciali...  \n",
       "4  payment history missing credit report made mis...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('complaints_processed.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_types = list(df['product'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(complaint_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162421, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_reporting       91179\n",
       "debt_collection        23150\n",
       "mortgages_and_loans    18990\n",
       "credit_card            15566\n",
       "retail_banking         13536\n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing\n",
    "    1. remove hyphens and accents\n",
    "    2. create customized stopwords list and remove stopwords\n",
    "    3. regex cleaning\n",
    "    4. typo correction with textblob\n",
    "    5. lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['narrative'] = df['narrative'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = preprocessing.make_pipeline(\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    preprocessing.remove.accents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))\n",
    "# removing some negative words from stopwords list\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_stopwords.remove('below')\n",
    "nltk_stopwords.remove(\"aren't\")\n",
    "nltk_stopwords.remove('couldn')\n",
    "nltk_stopwords.remove(\"couldn't\")\n",
    "nltk_stopwords.remove(\"didn't\")\n",
    "nltk_stopwords = list(nltk_stopwords)\n",
    "# add some abstract terms\n",
    "nltk_stopwords.append('like')\n",
    "nltk_stopwords.append('please')\n",
    "\n",
    "nltk_stopwords = set(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\AppData\\Local\\Temp\\ipykernel_4144\\868481445.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['narrative'] = df['narrative'].str.replace(r'\\b([a-z]|[A-Z]){15,}\\b', '', case=False)\n"
     ]
    }
   ],
   "source": [
    "# removing all words with more than 15 digits\n",
    "df['narrative'] = df['narrative'].str.replace(r'\\b([a-z]|[A-Z]){15,}\\b', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\AppData\\Local\\Temp\\ipykernel_4144\\1200782744.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['narrative'] = df['narrative'].str.replace(r'(.)\\1{3,}?', '', case=False)\n"
     ]
    }
   ],
   "source": [
    "# removing all characters that appear more than 2 times\n",
    "df['narrative'] = df['narrative'].str.replace(r'(.)\\1{3,}?', '', case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(sentence: str):\n",
    "    sentence = str(sentence)\n",
    "    to_ret = TextBlob(sentence)\n",
    "    to_ret = str(to_ret.correct())\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['narrative'] = df['narrative'].apply(spelling_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(str(sentence)))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LEMMA:\n",
    "    df['narrative'] = df['narrative'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['product']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. model preparation\n",
    "    1. tokenize\n",
    "    2. integer encoding\n",
    "    3. glove vectors\n",
    "    4. embedding matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(df['narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encoding\n",
    "from typing import List\n",
    "def get_max_token_length_per_doc(docs: List[List[str]])-> int:\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))\n",
    "\n",
    "# get the max length in terms of token length\n",
    "max_length = get_max_token_length_per_doc(df['narrative'])\n",
    "\n",
    "\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(df['narrative'], tokenizer)\n",
    "# this is a list of lists, the numbers represent the index position of that word.\n",
    "# for instance, 33 means the 33rd word in the vocabulary\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# glove vectors\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove\\glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= VOCAB_SIZE:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. model definition\n",
    "    1. LSTM\n",
    "    2. Bi-directional LSTM\n",
    "    3. RNN\n",
    "    4. BERT\n",
    "    5. Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer= \"adam\", metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 2,043,365\n",
      "Trainable params: 43,365\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_lstm_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "transfomed_labels = encoder.fit_transform(labels)\n",
    "# print(transfomed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, transfomed_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4569/4569 [==============================] - 1144s 250ms/step - loss: 0.4048 - accuracy: 0.8565 - val_loss: 0.4352 - val_accuracy: 0.8420\n",
      "Epoch 2/10\n",
      "4569/4569 [==============================] - 1174s 257ms/step - loss: 0.3645 - accuracy: 0.8703 - val_loss: 0.4136 - val_accuracy: 0.8484\n",
      "Epoch 3/10\n",
      "4569/4569 [==============================] - 1215s 266ms/step - loss: 0.3386 - accuracy: 0.8794 - val_loss: 0.4127 - val_accuracy: 0.8514\n",
      "Epoch 4/10\n",
      "4569/4569 [==============================] - 1249s 273ms/step - loss: 0.3188 - accuracy: 0.8857 - val_loss: 0.4087 - val_accuracy: 0.8527\n",
      "Epoch 5/10\n",
      "4569/4569 [==============================] - 1286s 281ms/step - loss: 0.3007 - accuracy: 0.8932 - val_loss: 0.4018 - val_accuracy: 0.8561\n",
      "Epoch 6/10\n",
      "4569/4569 [==============================] - 26073s 6s/step - loss: 0.2855 - accuracy: 0.8980 - val_loss: 0.4208 - val_accuracy: 0.8532\n",
      "Epoch 7/10\n",
      "4569/4569 [==============================] - 1386s 303ms/step - loss: 0.2729 - accuracy: 0.9029 - val_loss: 0.4194 - val_accuracy: 0.8523\n",
      "Epoch 8/10\n",
      "4569/4569 [==============================] - 1405s 308ms/step - loss: 0.2606 - accuracy: 0.9074 - val_loss: 0.4389 - val_accuracy: 0.8493\n",
      "Epoch 9/10\n",
      "4569/4569 [==============================] - 1495s 327ms/step - loss: 0.2487 - accuracy: 0.9119 - val_loss: 0.4605 - val_accuracy: 0.8418\n",
      "Epoch 10/10\n",
      "4569/4569 [==============================] - 1487s 325ms/step - loss: 0.2379 - accuracy: 0.9154 - val_loss: 0.4680 - val_accuracy: 0.8398\n",
      "Epoch 1/10\n",
      " 323/4061 [=>............................] - ETA: 2:22:04 - loss: 0.2395 - accuracy: 0.9180"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "if not USE_GPU:\n",
    "    with tf.device('/cpu:0'):\n",
    "        history = model.fit(padded_docs, transfomed_labels, validation_split = 0.1, epochs=10, verbose=1)\n",
    "else:\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # From https://keras.io/visualization/\n",
    "\n",
    "\n",
    "def plot_performance(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
